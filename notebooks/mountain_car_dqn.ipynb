{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment command below to kill current job:\n",
    "#!neuro kill $(hostname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"ml-recipe-mountain-car-dqn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQNs tend to “forget” certain $(s,a)=r$ situations it encountered in the past (kinda like catastrophic forgetting).\n",
    "\n",
    "Whole __experience replay__ idead is tp maintain a buffer of the old experiences and train it again on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device=torch.device(\"cpu\")):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mountain Car Environment</h2>\n",
    "\n",
    "A car is on a one-dimensional track, positioned between two \"mountains\".<br>\n",
    "The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass.<br>\n",
    "Therefore, the only way to succeed is to drive back and forth to build up momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"car\" controls>\n",
    "        <source src=\"https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/MountainCar-v0/original.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"MountainCar-v0\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Epsilon greedy exploration</h2>\n",
    "\n",
    "Greedy and Epsilon Greedy exploration methods are fairly easy to understand and to implement, but they suffer from major setback which is they have sub-optimal regret. As a matter of fact the regret of both Greedy and Epsilon Greedy grows linearly by the time.\n",
    "\n",
    "This is easy to intuitively understand, as the Greedy will lock on one action that happened to have good results at one point of time but it is not in reality the optimal action. So Greedy will keep exploiting this action while ignoring the others which might be better. It Exploits too much.\n",
    "\n",
    "The Epsilon Greedy on the other hand, explores too much because even when one action seem to be the optimal one, the methods keeps allocating a fixed percentage of the time for exploration.\n",
    "\n",
    "Conversely the decaying Epsilon Greedy methods, tries to decrease the percentage dedicated for exploration as time goes by.<br>\n",
    "Exploration probability is shown in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "def epsilon_by_frame(frame_idx):\n",
    "    return epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(10000)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep Q-Network</h2>\n",
    "\n",
    "DQN is a convolutional neural network, trained with a variant of Q-learning. It was first applied to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm.\n",
    "\n",
    "For more info we suggest reading the paper https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            action = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "model = model.to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(capacity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.LongTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).to(device)\n",
    "    done       = torch.FloatTensor(done).to(device)\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - expected_q_value.data).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses, env):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.subplot(133)\n",
    "    plt.title('environment')\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = False\n",
    "\n",
    "\n",
    "if do_training:\n",
    "    num_frames = 100000\n",
    "    batch_size = 32\n",
    "    gamma      = 0.99\n",
    "    plot_every = 200\n",
    "\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "\n",
    "    for frame_idx in range(1, num_frames + 1):\n",
    "        epsilon = epsilon_by_frame(frame_idx)\n",
    "        action = model.act(state, epsilon)\n",
    "\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            action = action.item()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state \n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss(batch_size)\n",
    "            wandb.log({'reward': episode_reward, 'loss': loss.data.item(),\n",
    "                      'environment': wandb.Image(env.render(mode='rgb_array'))})\n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "        if frame_idx % plot_every == 0:\n",
    "            plot(frame_idx, all_rewards, losses, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Next steps?,\t\t\n",
    "\n",
    "* Play around with actual Atari environments https://gym.openai.com/envs/#atari.\t\t\n",
    "* Consider using larger DQN architectures for harder games.\t\t\n",
    "* Explore more DQN tricks https://drive.google.com/file/d/0BxXI_RttTZAhVUhpbDhiSUFFNjg/view."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}